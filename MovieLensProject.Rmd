---
title: "Movie Lens Project"
author: "Lindsay Lee"
date: "2024-06-07"
output:
  pdf_document: default
  word_document: default
---
##########################################################
## edX Harvard Data Science Capstone: MovieLens Project
##########################################################

##**Introduction/Overview**

The MovieLens dataset (10 million version; Harper & Konstan, 2015) was originally collected by GroupLens Research to help understand people's preferences for movies across time (GroupLens, 2024). According to GroupLens (2024), this was a random sample of users that rated at least 20 movies (i.e., Action, Adventure, Animation, Children, Comedy, Crime, Documentary, Drama, Fantasy, Film-Noir, Horror, IMAX, Musical, Mystery, Romance, Sci-Fi, Thriller, War, Western). The current report sought to train a machine learning algorithm using inputs from a subset of the Movielens dataset (i.e., 10 Million version; Harper & Konstan, 2015) to predict movie ratings in the validation set. Specifically, I aimed to build a recommendation system based on how specific movie ID, user input, and genres predict movie ratings. The following research questions guided my inquiry: 
1. To what extent does genres and/or years relate to ratings for all movies included? 
2. To what extent does genres predict movie ratings?
3. To what extent does genres, user Id, and movie Id predict movie ratings?

##**Methods/Analysis**
The MovieLens 10 Million dataset was imported from GroupLens (2024) with provided by the edX Harvard R for Data Science Course (Irizarry, 2024). All analyses were conducted in RStudio R version 4.3.1 (R Core Team, 2023).


Prior to the splitting of the datasets, I explored the movie lens dataset by genres and ratings alone, then again by year. This required me to clean the movielens dataset prior to splitting the dataset to build a recommendation system. The genres variable specifically had over 700+ levels of and several levels were seperated by a space. Cleaning the genres variable reduced genres to 20 levels. 

This helped to answer the first research question on exploring the relationship of genres, year, and ratings using descriptive statistics and data visualization. 

For the second and third research questions, we used simple models with user ID, movie ID, and genre respectively. The outcome of interest was movie ratings by users (i.e., rating). The features of interest (predictors) were the specific users (userId), 20 different genres (genres), and the specific movies (movieId) that is also linked to the title of the movie (see documentation at Harper & Konstan, 2015). To build a recommendation system, there needs to be users who have provided ratings to specific items of interest (e.g., Netflix, Amazon; Irizarry, 2024). To assess ratings provided, I built a recommendation system with simple machine learning algorithms to assess the features (i.e., userId, movieId, genres) with the outcome (rating). Specifically, I was interested in how genres predicted movie ratings. Also, I was interested in how genres, user ID, and movie ID predicted movie ratings. 

Initial data were split from the movielens dataset into a training set(edx) and a validation set (final_holdout_test). Then, we assessed a model to look at the effect of combination of movie ID and user ID, and then another model to assess movie, user ID, and genres. Finally, to assess the robustness of our final model we used regularization to assess a final model of movie ID, user ID, and genres. 

Due to the nature of our outcome variable being "continuous", using a loss function was more appropriate for evaluation. Root Mean Square Error (RMSE) was used to evaluate model fit. RMSE measures the mean difference between the predicted values (training set) and the actual values (test set). It is calculated by computing by taking the average of the difference of the mean of a variable in the training set subtracted from the mean of same variable in the test set. Then, the square root is taken for the average difference squared (see Chapter 27.4.8: The loss function; Irizarry, 2024).

-----
```{r, echo = TRUE}
knitr::opts_chunk$set(echo = TRUE)
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
library(tidyverse)
library(caret)
library(tidyr)
library(ggplot2)
library(dplyr)


# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

options(timeout = 120)

dl <- "ml-10M100K.zip"
if(!file.exists(dl))
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings_file <- "ml-10M100K/ratings.dat"
if(!file.exists(ratings_file))
  unzip(dl, ratings_file)

movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
  unzip(dl, movies_file)

ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
                         stringsAsFactors = FALSE)

colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
ratings <- ratings |>
  mutate(userId = as.integer(userId),
         movieId = as.integer(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))

movies <- as.data.frame(str_split(read_lines(movies_file), fixed("::"), simplify = TRUE),
                        stringsAsFactors = FALSE)
colnames(movies) <- c("movieId", "title", "genres")
movies <- movies |>mutate(movieId = as.integer(movieId))

movielens <- left_join(ratings, movies, by = "movieId")

#convert timestamp to date then to year format
movielens <- mutate(movielens, date = as_datetime(timestamp))
movielens$year <- as.numeric(format(movielens$date, "%Y"))

#remove title, timestamp, and date 
movielens <- movielens |>
  select(-timestamp, -date)

# Convert factors
movielens$userId <- as.factor(movielens$userId)
movielens$movieId <- as.factor(movielens$movieId)

movielens <- movielens |>
  separate_rows(genres, sep = "\\|")

movielens = as.data.frame(movielens)
movielens$genres = as.factor(movielens$genres)

```

##**Results**

##*Research Question 1*
We can make a approximation that the accuracy of the ratings for movies is 3.53. 
```{r, echo = TRUE}
#Overall estimated Accuracy of ratings 
overallmean = mean(movielens$rating, na.rm = TRUE)
print(overallmean)
```

To assess the first research question, I initially analyzed descriptive statistics and visualizations of genres, year, and rating data in the movielens dataset. From the total MovieLens dataset, Drama (n = 4344198), Comedy(n = 3934068), & Action (n = 2845349) were the top 3 in the sheer quantity of included genres. 

```{r, echo = TRUE}
#Summarize mean and sd of rating by genres
genres_n = movielens |>group_by(genres) |>
  summarise(n = n(), mean = mean(rating, na.rm = TRUE), 
            sd = sd(rating, na.rm = TRUE)) |> arrange(desc(n))

#kable can create clean tables in R Markdown
knitr::kable(genres_n, summary=TRUE, rownames=TRUE)

#drop no genres listed as a category, only 7 
movielens = subset(movielens, genres != "(no genres listed)")
movielens  = droplevels(movielens)

```
After filtering by the mean rating from the MovieLens dataset, we find that Film-Noir (m = 4.01, sd = .89), Documentary (m = 3.78, sd = 1), and War (m = 3.78, sd = 1.01) have the highest mean ratings. When we visually inspect genres and ratings, we do not see large differences in ratings, yet it does verify the same findings from the summary table. This led me to question how genres were impacting the ratings, how time (years) impacted ratings, and more specifically, how users (userId) were rating specific movies (movieId) based on genres (see Research Question 2 & 3 for building my recommendation system). 
```{r, echo = TRUE}
#Summarize mean and sd of rating by genres
genres_m = movielens |>group_by(genres) |>
  summarise(n = n(), mean = mean(rating, na.rm = TRUE), 
            sd = sd(rating, na.rm = TRUE)) |> arrange(desc(mean))

#kable can create clean tables in R Markdown
knitr::kable(genres_m, summary=TRUE, rownames=TRUE)

#Explore bar plots of Genres x Rating 
movielens |>
  ggplot(aes(x = genres, y = rating, fill = genres)) + 
  geom_bar(stat = "summary", fun = "mean", fill = "darkblue") + 
  theme(axis.text.x = element_text(angle = 30))

```

When filtering by year and rating, we see that Drama, Comedy, Action, and Thriller seem to be frequent. It does not appear that there is a clear pattern of how year is influencing ratings. To visualize the relationship between genres, year, and mean rating, see Figure below. Across time, genres did not consistently predict mean ratings. 
```{r, echo = TRUE}
#Summarize mean, sd, and count by year and genres
#make year a factor type to run in ggplot plot
movielens$year = as.factor(movielens$year)
year_meanrating = movielens |>
  group_by(genres, year) |>
  summarise(n = n(), meanrating = mean(rating, na.rm = TRUE, 
                                       sd = sd(rating, na.rm = TRUE)))

knitr::kable(year_meanrating, summary=TRUE, rownames=TRUE)

#Explore genres and year by ratings 
ggplot(movielens, aes(x = year, y = meanrating, group = genres, color = genres)) +
  geom_smooth(data = year_meanrating, se = FALSE, method = "lm") + 
  theme(axis.text.x = element_text(angle = 30)) 

```


#Research Question 2 & 3
To build a recommendation system, we used the code provided in the Introduction to Data Science textbook (Irizarry, 2024) and the edX Harvard R for Data Science Course, however provided some modifications to explore the data visually and to add additional models that included genres (which was not used in the course). 
```{r, echo = TRUE}
library(caret)
# Final hold-out test set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

#Make sure userId and movieId in final hold-out test set are also in edx set
#use this at the end to test the model
final_holdout_test <- temp |>
  semi_join(edx, by = "movieId") |>
  semi_join(edx, by = "userId")
```

Then, prior to analysis, we removed the temp and final_holdout_test set from the edx dataset and removed the other objects related to the entire dataset. 
```{r, echo = TRUE}
#Add rows removed from final hold-out test set back into edx set
removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

The estimated accuracy of the training and test set resemble the movielens dataset around 3.52.
```{r, echo = TRUE, out.width = '10%'}
#What is the estimated Accuracy of each set?
mean(edx$rating, na.rm = TRUE)
mean(final_holdout_test$rating, na.rm = TRUE)
```

I ran summary statistics and visually inspected both of the training and test set to verify that their distributions of genres and ratings look similar. 
```{r, echo = TRUE, out.width = '10%'}
library(ggplot2)
#Summarize count, mean, sd
genres_edx = edx |>group_by(genres) |>
  summarise(n = n(), mean = mean(rating, na.rm = TRUE), 
            sd = sd(rating, na.rm = TRUE)) |> 
           arrange(desc(mean))

#kable can create clean tables in R Markdown
knitr::kable(genres_edx, summary=TRUE, rownames=TRUE)

edx |>group_by(genres, rating) |>
  ggplot(aes(x = rating, fill = genres)) + 
  scale_y_continuous(trans='sqrt') +
  geom_bar() + 
  theme(axis.text.x = element_text(angle = 30)) 

ggplot(edx, aes(x = genres, y = rating)) + 
  geom_bar(stat = "summary", fun = "mean") + theme(axis.text.x = element_text(angle = 30)) 

```

```{r, echo = TRUE}
#Visually, the training and final hold out sets look similar
#plots are based on count of ratings, did a square root transformation on ratings on both the edx and final holdout test
genres_final_holdout_test = final_holdout_test |>group_by(genres) |>
  summarise(n = n(), mean = mean(rating, na.rm = TRUE), 
            sd = sd(rating, na.rm = TRUE)) |> arrange(desc(mean))
#kable can create clean tables in R Markdown
knitr::kable(genres_final_holdout_test, summary=TRUE, rownames=TRUE)

final_holdout_test |>group_by(genres, rating) |>
  ggplot(aes(x = rating, fill = genres)) +
  scale_y_continuous(trans='sqrt') +
  geom_bar() +  theme(axis.text.x = element_text(angle = 30)) 

ggplot(final_holdout_test, aes(x = genres, y = rating)) + 
  geom_bar(stat = "summary", fun = "mean") + theme(axis.text.x = element_text(angle = 30)) 

```

To assess the recommendation system, we will use RMSE to evaluate performance of the each algorithm. We will build the recommendation system hierarchically, first with the Average, then each variable individually Movie, then User, then Genre. Then, we will run the Movie & User Effects Model and then finally the interaction of the Movie, User, & Genre Effects Model. The following code was adapted from the edX Harvard R for Data Science course. 

```{r, echo = TRUE}
#Building the Recommendation System-----------
#Define RMSE function -----------
#Use the RMSE function defined in the course
#https://learning.edx.org/course/course-v1:HarvardX+PH125.8x+1T2023/block-v1:HarvardX+PH125.8x+1T2023+type@sequential+block@7e7727ce543b4ed6ae6338626862eada/block-v1:HarvardX+PH125.8x+1T2023+type@vertical+block@55f470e389ca4d119b1e311840be15e7
#https://learning.edx.org/course/course-v1:HarvardX+PH125.8x+1T2023/block-v1:HarvardX+PH125.8x+1T2023+type@sequential+block@7e7727ce543b4ed6ae6338626862eada/block-v1:HarvardX+PH125.8x+1T2023+type@vertical+block@df3d8a86b43f4247a4dd42bcabb1a663
#https://rafalab.dfci.harvard.edu/dsbook/large-datasets.html#modeling-movie-effects 

RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))}

mu_hat = mean(edx$rating, na.rm = TRUE)
mu_hat
hist(edx$rating)
#same plot but in ggplot
edx |>
  ggplot(aes(x = rating))  +
  geom_bar() + theme_bw()

#use this for testing later
naive_rmse <- RMSE(final_holdout_test$rating, mu_hat)
naive_rmse

predictions <- rep(2.5, nrow(final_holdout_test))
RMSE(final_holdout_test$rating, predictions)
rmse_results <- tibble(method = "Average", RMSE = naive_rmse)
```

The movie effect model yielded improved performance compared to the average effect model with RMSE at .94. 
```{r, echo = TRUE}
#Model ONLY Movie effects -----------
mu <- mean(edx$rating) 
movie_avgs <- edx |>
  group_by(movieId) |>
  summarize(b_i = mean(rating - mu))
movie_avgs %>%
  qplot(b_i, geom ="histogram", bins = 40, data = ., color = I("black"))

predicted_ratings <- mu + final_holdout_test |>
  left_join(movie_avgs, by='movieId') |>
  pull(b_i)

model_1_rmse <- RMSE(predicted_ratings, final_holdout_test$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie Effect Model",
                                     RMSE = model_1_rmse ))
rmse_results |>knitr::kable()

```


The user effect model yielded an EMSE of .966, which showed worse performance than the movie effect model. 
```{r, echo = TRUE}
#Modeling ONLY Users Effects--------
#This modeled all users who rated all movies
user_avgs <- edx |>
  group_by(userId) |>
  summarize(b_k = mean(rating - mu)) 
user_avgs %>%
  qplot(b_k, geom ="histogram", bins = 25, data = ., color = I("black")) 

predicted_ratings <- mu + final_holdout_test |>
  left_join(user_avgs, by='userId') |>
  pull(b_k)

model_2_rmse <- RMSE(predicted_ratings, final_holdout_test$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="User Effect Model",
                                     RMSE = model_2_rmse ))
rmse_results |>knitr::kable()

```

The genre effect model yielded poor performance on its own. With an RMSE at around 1.04. 
```{r, echo = TRUE}
#Modeling ONLY Genre Effects-------
#group by all genres and count in the training set 
edx |>group_by(genres, rating) |>
  summarize(genres_count = n())

genre_avgs <- edx |>
  group_by(genres) |>
  summarize(b_z = mean(rating - mu))

genre_avgs %>%
  qplot(b_z, geom ="histogram", bins = 20, data = ., color = I("black")) 

predicted_ratings_genre <- mu + final_holdout_test |>
  left_join(genre_avgs, by='genres') |>
  pull(b_z)

model_3_rmse <- RMSE(predicted_ratings_genre, final_holdout_test$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Genre Effect Model",
                                     RMSE = model_3_rmse ))
rmse_results |>knitr::kable()
```

This model assessed movie and user effects and yielded an improved RMSE at .857
```{r, echo = TRUE}
#Hierarchically Add Movie & User Effect -----------
# Modeling Movie & User Effects ----------------
edx |>
  group_by(userId) |>
  summarize(b_u = mean(rating)) |>
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black")+ theme_bw()

user_avgs <- edx |>
  left_join(movie_avgs, by='movieId') |>
  group_by(userId) |>
  summarize(b_u = mean(rating - mu - b_i))

predicted_ratings <- final_holdout_test |>
  left_join(movie_avgs, by='movieId') |>
  left_join(user_avgs, by='userId') |>
  mutate(pred = mu + b_i + b_u) |>
  pull(pred)

model_4_rmse <- RMSE(predicted_ratings, final_holdout_test$rating)

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Effects Model",  
                                     RMSE = model_4_rmse ))
rmse_results |>knitr::kable()
```

The final model explored genre, movie, and user effects. Similar to the course (Irizarry, 2024), we filtered by users who rated over 100. This yielded the best performance with RMSE = .856
```{r, echo = TRUE}
#Model Genre x Movie x User Effects -------------------
edx |>
  group_by(genres) |>
  summarize(b_z = mean(rating)) |>
  ggplot(aes(b_z)) + 
  geom_histogram(bins = 30, color = "black")

genre_avgs <- edx |>
  left_join(movie_avgs, by='movieId') |>
  left_join(user_avgs, by ='userId') |>
  group_by(genres) |>
  summarize(b_final = mean(rating - mu - b_i - b_u))

predicted_ratings <- final_holdout_test |>
  left_join(movie_avgs, by='movieId') |>
  left_join(user_avgs, by='userId') |>
  left_join(genre_avgs, by = 'genres') |>
  mutate(pred = mu + b_i + b_u + b_final) |>
  pull(pred)

model_5_rmse <- RMSE(predicted_ratings, final_holdout_test$rating)

rmse_results <- bind_rows(rmse_results,
                         tibble(method="Movie + User + Genre Effects Model",  
                                     RMSE = model_5_rmse ))
rmse_results |>knitr::kable()
```

#Conclusion 
Exploring the MovieLens dataset provided insight that genres was a predictor to continue to explore in relation to ratings, and that year was justifiably excluded in the dataset between genres and ratings. After building a recommendation system, we found that both the User & Movie Effect Model and the Genre, User, & Movie Effect yielded models with the lowest RMSE < 0.8649. The final model that yielded slightly better performance was the Genre, User, & Movie Effect Model (RMSE = .856) than the User & Movie Effect model with (RMSE = .857). 

#Limitations & Future Directions
The following report was limited to only user ID, movie ID, genres, titles, and year, however other model effects should take into account specific user characteristics that could impact ratings. There is also the risk of overfitting the model, which would need to be compared to the larger MovieLens dataset for verification. 

We also used simple machine learning algorithms to develop a recommendation system. Future research should explore different machine learning algorithms to improve prediction and performance (e.g., random forest algorithms or neural networks). Also, I'd like to explore specific genres as predictors in a different machine learning algorithm (e.g., kNN). To further explore trends across time, I'd like to also use multilevel modeling to look at different clusters of data based on user characteristics. 

##References
Grouplens. (2024). MovieLens. https://grouplens.org/datasets/movielens/

Harper, M. F., & Konstan, J. A. (December, 2015). The MovieLens datasets: History and context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19. http://dx.doi.org/10.1145/2827872

Irizarry, R. A. (2024). Introduction to data science: Data analysis and prediction algorithms with R.
https://rafalab.dfci.harvard.edu/dsbook/large-datasets.html#recommendation-systems

Kuhn, M. et al. (2023). caret: Classification and regression training. R package version 6.0-94. https://cran.r-project.org/web/packages/caret/index.html

Wickham, H., Vaughan, D., & Girlich, M. (2024). tidyr: Tidy messy data. R package version 1.3.1, https://github.com/tidyverse/tidyr, https://tidyr.tidyverse.org

Wickham et al. (2014). dplyr. https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html 
